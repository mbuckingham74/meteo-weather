# Database Schema Optimization Recommendations

**Analysis Date:** November 6, 2025
**Database:** MySQL 8.0
**Current Size:** 585K+ weather records
**Cache Hit Rate:** 99%

---

## üìä Current Schema Analysis

### Strengths ‚úÖ
- Good use of indexes on frequently queried columns
- Proper foreign key relationships with CASCADE
- UNIQUE constraints prevent duplicate data
- InnoDB engine with utf8mb4 character set
- Composite indexes on multi-column queries
- JSON columns for flexible data storage

### Areas for Improvement üîß

---

## üéØ High Priority Optimizations

### 1. **Add Full-Text Search for Locations** ‚≠ê‚≠ê‚≠ê

**Current Issue:**
```sql
-- Current query uses LIKE with wildcards (slow on large datasets)
SELECT * FROM locations
WHERE city_name LIKE '%seattle%' OR country LIKE '%seattle%'
```

**Problem:** `LIKE '%term%'` cannot use regular indexes and requires full table scan.

**Solution:** Add FULLTEXT index

```sql
-- Migration: Add FULLTEXT index
ALTER TABLE locations
ADD FULLTEXT INDEX idx_fulltext_search (city_name, country, state);

-- Updated query (10-100x faster)
SELECT * FROM locations
WHERE MATCH(city_name, country, state) AGAINST('seattle' IN NATURAL LANGUAGE MODE)
LIMIT 10;
```

**Impact:**
- 10-100x faster location searches
- Better relevance scoring
- No full table scans

**Files to Update:**
- `backend/services/locationService.js:searchLocations()`
- `backend/services/historicalDataService.js:searchLocationsByName()`

---

### 2. **Optimize weather_data Table for Large Datasets** ‚≠ê‚≠ê‚≠ê

**Current Issue:** With 585K+ records and growing, table will become slow.

**Problem Areas:**
- No partitioning strategy for time-series data
- TEXT field (`weather_description`) stored inline
- BIGINT AUTO_INCREMENT will eventually be too large

**Solutions:**

#### A. Partition by Date Range (Recommended)
```sql
-- Create partitioned table (MySQL 8.0+)
ALTER TABLE weather_data
PARTITION BY RANGE (YEAR(observation_date)) (
    PARTITION p2020 VALUES LESS THAN (2021),
    PARTITION p2021 VALUES LESS THAN (2022),
    PARTITION p2022 VALUES LESS THAN (2023),
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p2024 VALUES LESS THAN (2025),
    PARTITION p2025 VALUES LESS THAN (2026),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);
```

**Benefits:**
- Queries for specific date ranges only scan relevant partitions
- Easier to archive/drop old partitions
- Better query performance (up to 10x for date-range queries)

#### B. Normalize `weather_description`
```sql
-- Create lookup table for weather descriptions
CREATE TABLE IF NOT EXISTS weather_conditions_lookup (
    id SMALLINT AUTO_INCREMENT PRIMARY KEY,
    condition_code VARCHAR(50) NOT NULL UNIQUE,
    description TEXT NOT NULL,
    INDEX idx_code (condition_code)
) ENGINE=InnoDB;

-- Modify weather_data to reference lookup
ALTER TABLE weather_data
    DROP COLUMN weather_description,
    ADD COLUMN condition_id SMALLINT,
    ADD FOREIGN KEY (condition_id) REFERENCES weather_conditions_lookup(id);
```

**Benefits:**
- Reduces row size (TEXT ‚Üí SMALLINT = ~98% reduction)
- Faster table scans
- Consistent descriptions across database

#### C. Add Covering Index for Common Queries
```sql
-- Add covering index for date-range queries
CREATE INDEX idx_location_date_temp
ON weather_data (location_id, observation_date, temperature_avg, temperature_high, temperature_low);
```

**Impact:**
- 585K records ‚Üí Can handle 10M+ with partitioning
- 30-50% reduction in table size with normalization
- 5-10x faster queries on large datasets

---

### 3. **Optimize `api_cache` Table** ‚≠ê‚≠ê

**Current Issue:** `VARCHAR(255)` for `cache_key` with MD5 hash (32 chars).

**Problem:** Wasting 223 bytes per row.

**Solution:**
```sql
-- Change cache_key to exact size
ALTER TABLE api_cache
MODIFY COLUMN cache_key CHAR(32) NOT NULL;

-- Add cache cleanup job (in code)
-- Delete expired entries daily
DELETE FROM api_cache WHERE expires_at < DATE_SUB(NOW(), INTERVAL 1 DAY);
```

**Also Consider:** Store hash as BINARY(16) instead of CHAR(32)
```sql
ALTER TABLE api_cache
MODIFY COLUMN cache_key BINARY(16) NOT NULL;

-- Update code to use binary hash
-- const hash = crypto.createHash('md5').update(key).digest(); // Buffer instead of hex
```

**Impact:**
- 223 bytes saved per cached response
- With 10K cache entries: 2.2MB saved
- Faster index lookups (shorter keys)

---

### 4. **Add Composite Index for Climate Stats** ‚≠ê‚≠ê

**Current Query Pattern:**
```sql
SELECT * FROM climate_stats
WHERE location_id = ? AND month IN (1,2,3,4,5,6,7,8,9,10,11,12);
```

**Problem:** Current index `idx_location_month` is good, but queries often need all months.

**Solution:**
```sql
-- Add index for month-range queries
CREATE INDEX idx_location_year_range
ON climate_stats (location_id, data_year_start, data_year_end);
```

**Impact:** Faster climate statistics queries, especially for multi-year averages.

---

### 5. **Optimize `shared_ai_answers` for Cleanup** ‚≠ê‚≠ê

**Current Issue:** No automatic cleanup of expired shares.

**Solution A: Database Event (Recommended)**
```sql
-- Create event to auto-delete expired shares
DELIMITER //
CREATE EVENT IF NOT EXISTS cleanup_expired_shares
ON SCHEDULE EVERY 1 DAY
STARTS CURRENT_TIMESTAMP
DO
BEGIN
    DELETE FROM shared_ai_answers
    WHERE expires_at < DATE_SUB(NOW(), INTERVAL 1 DAY);
END //
DELIMITER ;

-- Enable event scheduler
SET GLOBAL event_scheduler = ON;
```

**Solution B: Add Soft Delete**
```sql
-- Add soft delete column
ALTER TABLE shared_ai_answers
ADD COLUMN is_deleted BOOLEAN DEFAULT FALSE,
ADD INDEX idx_active (is_deleted, expires_at);

-- Query only active shares
SELECT * FROM shared_ai_answers
WHERE share_id = ? AND is_deleted = FALSE AND expires_at > NOW();
```

**Impact:** Prevents table bloat, maintains performance over time.

---

## üîÑ Medium Priority Optimizations

### 6. **Add Indexes for User Queries** ‚≠ê

**Current Missing Indexes:**

```sql
-- Add index for user login lookups (email + password check)
-- Already exists: INDEX idx_email (email)
-- But consider:
CREATE INDEX idx_email_active ON users (email, last_login);

-- Add index for recent logins (for analytics)
CREATE INDEX idx_last_login ON users (last_login DESC);

-- Add index for token expiry cleanup
CREATE INDEX idx_token_expiry ON refresh_tokens (expires_at);
```

---

### 7. **Optimize coordinate searches** ‚≠ê

**Current Query:**
```sql
SELECT * FROM locations
WHERE ABS(latitude - ?) < ? AND ABS(longitude - ?) < ?
LIMIT 1;
```

**Problem:** `ABS()` function prevents index usage.

**Solution A: Add Spatial Index (MySQL 8.0+)**
```sql
-- Add POINT column for spatial indexing
ALTER TABLE locations
ADD COLUMN coordinates POINT NOT NULL,
ADD SPATIAL INDEX idx_spatial (coordinates);

-- Update existing data
UPDATE locations
SET coordinates = POINT(longitude, latitude);

-- Updated query (much faster)
SELECT * FROM locations
WHERE ST_Distance_Sphere(
    coordinates,
    POINT(?, ?)
) < ?
LIMIT 1;
```

**Solution B: Add Bounding Box Index**
```sql
-- Create composite index for coordinate range queries
CREATE INDEX idx_lat_lon_range
ON locations (latitude, longitude);

-- Rewrite query to use range instead of ABS
SELECT * FROM locations
WHERE latitude BETWEEN ? AND ?
  AND longitude BETWEEN ? AND ?
LIMIT 1;
```

**Impact:** 10-50x faster coordinate lookups.

---

### 8. **Optimize JSON Storage** ‚≠ê

**Current Usage:** JSON columns in `api_cache`, `shared_ai_answers`.

**Optimization:**
```sql
-- Add virtual generated columns for frequently queried JSON fields
ALTER TABLE shared_ai_answers
ADD COLUMN location_extracted VARCHAR(255)
    GENERATED ALWAYS AS (JSON_UNQUOTE(JSON_EXTRACT(weather_data, '$.location'))) STORED,
ADD INDEX idx_location_extracted (location_extracted);
```

**Impact:** Faster filtering on JSON fields without full JSON parsing.

---

## ‚ö° Low Priority / Future Optimizations

### 9. **Consider Read Replicas**

**When:** Database grows beyond 10M records or high read load.

**Setup:**
- Master: Write operations
- Replica(s): Read operations (historical data, climate stats)

**Impact:** 2-10x read capacity.

---

### 10. **Add Query Result Caching**

**Solution:**
```sql
-- Enable MySQL query cache (if not using MariaDB 10.2+)
SET GLOBAL query_cache_type = ON;
SET GLOBAL query_cache_size = 268435456; -- 256MB
```

**Note:** MySQL 8.0+ removed query cache. Consider using Redis instead.

---

### 11. **Archive Old Data**

**Strategy for `weather_data`:**
```sql
-- Create archive table
CREATE TABLE weather_data_archive LIKE weather_data;

-- Move data older than 2 years to archive
INSERT INTO weather_data_archive
SELECT * FROM weather_data
WHERE observation_date < DATE_SUB(NOW(), INTERVAL 2 YEAR);

DELETE FROM weather_data
WHERE observation_date < DATE_SUB(NOW(), INTERVAL 2 YEAR);
```

**Impact:** Keep active table small and fast.

---

## üìä Expected Performance Improvements

| Optimization | Current Time | Optimized Time | Improvement |
|--------------|--------------|----------------|-------------|
| Location search (`LIKE`) | 50-200ms | 5-10ms | **10-20x faster** |
| Weather data queries | 100-500ms | 10-50ms | **10x faster** |
| Coordinate lookup | 50-100ms | 1-5ms | **50x faster** |
| Climate stats | 50-100ms | 10-20ms | **5x faster** |
| Cache lookups | 5-10ms | 2-5ms | **2x faster** |

**Overall Impact:** 5-20x performance improvement on common queries.

---

## üõ†Ô∏è Implementation Priority

### Phase 1: Quick Wins (1-2 hours)
1. ‚úÖ Add FULLTEXT index for locations
2. ‚úÖ Optimize `cache_key` VARCHAR size
3. ‚úÖ Add missing indexes (tokens, user queries)
4. ‚úÖ Create auto-cleanup event for expired shares

### Phase 2: Major Improvements (4-6 hours)
1. ‚úÖ Partition `weather_data` table by year
2. ‚úÖ Add spatial index for coordinate searches
3. ‚úÖ Normalize weather descriptions
4. ‚úÖ Add covering indexes

### Phase 3: Advanced (Future)
1. ‚è≥ Read replicas (when needed)
2. ‚è≥ Data archival strategy (> 10M records)
3. ‚è≥ Redis caching layer (optional)

---

## üìù Migration Scripts

### Safe Migration Strategy

```bash
# 1. Backup database
mysqldump -u root -p meteo_db > backup_$(date +%Y%m%d).sql

# 2. Test on staging/local first
mysql -u root -p meteo_db_staging < migration.sql

# 3. Run during low-traffic period
# 4. Monitor query performance after migration
```

### Migration Script Template

```sql
-- migration_001_optimize_locations.sql
START TRANSACTION;

-- Add FULLTEXT index
ALTER TABLE locations
ADD FULLTEXT INDEX idx_fulltext_search (city_name, country, state);

-- Add spatial index
ALTER TABLE locations
ADD COLUMN coordinates POINT,
ADD SPATIAL INDEX idx_spatial (coordinates);

UPDATE locations
SET coordinates = POINT(longitude, latitude);

ALTER TABLE locations
MODIFY COLUMN coordinates POINT NOT NULL;

COMMIT;
```

---

## üéØ Recommended Implementation Order

**Immediate (This Week):**
1. Add FULLTEXT index for location searches
2. Optimize `api_cache` cache_key size
3. Add cleanup event for expired shares

**Short Term (This Month):**
4. Partition `weather_data` by year
5. Add spatial index for coordinates
6. Add covering indexes for common queries

**Long Term (As Needed):**
7. Normalize weather descriptions (when size > 1GB)
8. Read replicas (when load > 1000 QPS)
9. Data archival (when data > 10M records)

---

## üìö Additional Resources

- **MySQL Partitioning:** https://dev.mysql.com/doc/refman/8.0/en/partitioning.html
- **Full-Text Search:** https://dev.mysql.com/doc/refman/8.0/en/fulltext-search.html
- **Spatial Data:** https://dev.mysql.com/doc/refman/8.0/en/spatial-types.html
- **Index Optimization:** https://dev.mysql.com/doc/refman/8.0/en/optimization-indexes.html

---

## üö® Important Notes

1. **Always backup before migrations**
2. **Test on staging environment first**
3. **Run migrations during low-traffic periods**
4. **Monitor query performance after changes**
5. **Keep old indexes until new ones are proven faster**

---

**Questions or need help implementing?** See [docs/development/](development/) for guides.

**Last Updated:** November 6, 2025
